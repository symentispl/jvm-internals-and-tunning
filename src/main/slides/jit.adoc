== JIT me baby one more time

From previous section, we know what Java bytecode is interpreted
(actually interpreter is generated from template, but it isn't the point)

Interpreted languages are know for being slow +
(like Python, Ruby, Lua. PHP).

What makes Java fast?

== what's jit

Just In Time compiler, +
is a technique of code compilation during runtime, +
as opposed to Ahead Of Time compilation +
like C or C++, or Go or Rust

[%notille]
== Smalltalk and lisp

first time used in LISP and than later on expanded during development of
Smalltalk +
(including Strongtalk and Self languages)

== JVM is not revolution its evolution, baby


[%notitle]
== we fucked up

image::Cif5ryKUUAAiSSJ.jpg[background, size=contain]

[role=highlight_title]
== it's huge

image::lhc10.jpg[background, size=cover]

[%notitle]
=== JDK SLOC

image::JDK9SLOC.png[]

[role=highlight_title]
== it's complex

image::cms_detector_web_1024.jpg[background, size=cover]

[role=highlight_title]
== it's fast

image::lhc-particle-collision-523875355-f.jpg[background, size=cover]

[%notitle]
=== fibonacci code

[source,java]
----
public static long fibonacci(int n) {
    if (n <= 1) return n;
    else return fibonacci(n-1) + fibonacci(n-2);
}
----

[%notitle]
=== fibonacci interpreted mode

*# interpreted mode only*

 $> java -cp target/classes -Xint pl.symentis.jit.Fibonacci
 execute time is 2900 ms
 result is 46368

[%notitle]
=== fibonacci compiled mode

*# mixed mode (with JIT compiler)*

 $> java -cp target/classes pl.symentis.jit.Fibonacci
 execute time is 67 ms
 result is 46368

[role="highlight_title"]
== it's magic

image::bbb.jpg[background, size=cover]

== a little bit of history

* HotSpot JVM, default JVM since Java 1.3
* a tale of two compilers C1 and C2, aka "client" and "server"

=== C1

* it's a fast compiler
* uses linear register allocation
* settings can be found in https://github.com/openjdk/jdk11/blob/master/src/hotspot/share/c1/c1_globals.hpp[src/hotspot/share/c1/c1_globals.hpp]

=== C1 inline settings

`C1MaxTrivialSize=6`:: The maximum bytecode size of a trivial method to be inlined by C1 (level 1)
`C1MaxInlineSize=35`:: The maximum bytecode size of a method to be inlined by C1 (level 3)
`C1MaxInlineLevel=9`:: The maximum number of nested calls that are inlined by C1

=== C2

* is slow, but generates faster code (compared to C1)
* uses graph coloring algorithm for register allocation
* based on templates, called Architecture Desciption Language
* settings can be found in https://github.com/openjdk/jdk11/blob/master/src/hotspot/share/opto/c2_globals.hpp[src/hotspot/share/opto/c2_globals.hpp]

=== C2 settings

`MaxInlineLevel=15`:: maximum number of nested calls that are inlined by high tier compiler
`MaxInlineSize=35`:: The maximum bytecode size of a method to be inlined by high tier compiler (WARNING!!!)
`FreqInlineSize=325`:: The maximum bytecode size of a frequent method to be inlined


=== tiered compliation

enabled by default since Java 8, +
uses both compilers for better JVM start up time

because JVM suffered from so-called _warmups_, +
it didn't solve the problem, +
it just made it less annoying

[role=highlight_title]
== any questions?

image::kdLCmqOWPMOSQ.gif[background, size=cover]

== question number one

when code is compiled?

[%notitle]
=== print compilation fibonacci

*-Xlog:jit+compilation=debug Fibonacci*

[source]
--
[0,080s][debug][jit,compilation]   36   !   3       java.util.concurrent.ConcurrentHashMap::putVal (432 bytes)
[0,080s][debug][jit,compilation]   40     n 0       jdk.internal.misc.Unsafe::compareAndSetLong (native)   
[0,081s][debug][jit,compilation]   42     n 0       jdk.internal.misc.Unsafe::compareAndSetObject (native)   
[0,084s][debug][jit,compilation]   37       3       java.util.concurrent.ConcurrentHashMap::addCount (289 bytes)
[0,086s][debug][jit,compilation]   39       3       java.util.concurrent.ConcurrentHashMap::spread (10 bytes)
--

=== compilation attributes

* %: The compilation is OSR
* s: The method is synchronized
* !: The method has an exception handler
* b: Compilation occurred in blocking mode
* n: Compilation occurred for a wrapper to a native method

=== compilation level

* 0: interpreter
* 1: C1 with full optimization (no profiling)
* 2: C1 with invocation and backedge counters
* 3: C1 with full profiling (level 2 and `MethodData`)
* 4: C2 Godspeed You!

=== profiling?

[%notitle]
=== profiling explained

JIT (and interpreter) +
use `MethodCounter` and `MethodData` +
to record invocations and other profiler data +
(backedges, call sites profile, taken branches)


=== !

https://github.com/openjdk/jdk11/blob/master/src/hotspot/share/oops/methodData.hpp[src/hotspot/share/oops/methodData.hpp]


=== which leads to a next question

== question number two

what code gets compiled?

=== a hot code

a code which has reached invocation thresholds

2000 invocations for C1 +
10000 invocations for C2

(and trivial methods)

=== compiler flags

`-XX:+TieredCompilation`:: Enables tiered compilation
`-XX:+TieredStopAtLevel=`:: Stop at given compilation level
`-XX:Tier4InvocationThreshold=`,`-XX:Tier3MinInvocationThreshold=`:: minimum invocation of methods at which compiler is invoked
`Tier3BackEdgeThreshold`,`Tier4BackEdgeThreshold`:: Back edge threshold at which tier OSR compilation is invoked

== compiler policy

of course, it is all more complex

https://stackoverflow.com/questions/35601841/how-does-the-jvm-decided-to-jit-compile-a-method-categorize-a-method-as-hot[How does the JVM decided to JIT-compile a method (categorize a method as "hot")?]

or 

https://github.com/openjdk/jdk11/blob/master/src/hotspot/share/runtime/simpleThresholdPolicy.hpp[src/hotspot/share/runtime/simpleThresholdPolicy.hpp]

=== compiler queues and workers

HotSpot runs set of threads that compile your code in a background +
(this setting can be adjusted with `-XX:-BackgroundCompilation`)

Compilation occurs asynchronously for methods that are placed on the compilation queue.

The queue is not strictly ordered; hot methods are compiled before other methods in the queue. This is another reason compilation IDs can appear out of order in the compilation log.

=== !

Number of total compiler threads can be controled with `-XX:CICompilerCount=N` (minimum value is 2) +

Default for 8 cores is 1 C1 thread and 2 C2 threads.

== code cache

Compiled code is hold in a special memory region called `code cache`, which has a fixed size, once it has filled up, JVM will not be able to compile any additional code.

=== !

InitialCodeCacheSize:: sets initial size of code cache
ReservedCodeCacheSize:: sets maximum size of code cache
SegmentedCodeCache:: code cache is divided into segments (holding different types of compiled code)

=== !

In Java 11, the code cache is segmented into three parts:

* Nonmethod code
* Profiled code
* Nonprofiled code

This was introduced in Java 9, https://openjdk.java.net/jeps/197[JEP 197: Segmented Code Cache]

== question number three

what does it make it so fast?

=== optimizations

optimizations are driven by hardware +
which is designed +
with these two design constraints in mind

[role="highlight_title"]
== the world is a magnetic tape

image::audio-cassette.jpg[background, size=cover]

[%notitle]
=== inlining

*inlining* +
*branch prediction* +
*type profile* (specific for OO languages)

[role="highlight_title"]
== data locality

image::messy-desk_2637008b.jpg[background, size=cover]

[%notitle]
=== escape analisys

*escape analisys* +
*&* +
*register allocation*

[role="highlight_title"]
== inlining: expanding optimizations horizon

image::horizon-013.jpg[background]

[%notitle]
=== null check folding example

[source,java]
----
public static void assertNotNull(Object obj) {
  if (obj == null) {
    out.println(format("%s is null", obj));
  }
}

public void nullCheckFolding() {
  assertNotNull(this);
}
----

=== !

don't mix it with null check elimination +
(in next slides)

[%notitle]
=== null check folding flags

to trace inlining add this command line switch,

`-Xlog:jit+inlining=debug`

[%notitle]
=== after inline

[source,java]
----
public void nullCheckFolding() {
  if (this == null) {
    out.println(format("%s is null", obj));
  }
}
----

[%notitle]
=== null check folding

[source,java]
----
public void nullCheckFolding() {
  if (false) {
    out.println(format("%s is null", obj));
  }
}
----

[%notitle]
=== dead code elimination

[source,java]
----
public void nullCheckFolding() {
}
----

[role="highlight_title]
== we need to go deeper

image::cave-light-2.gif[background, size=cover]

=== eye of the beholder

HotSpot DISassembler aka hsdis and compiler flags

[%notitle]
=== null check folding assembly flags

  -XX:+UnlockDiagnosticVMOptions +
  -Xlog:jit+inlining=debug
  -XX:CompileCommand="print,*NullCheckFolding.nullCheckFolding"

=== PrintAssembly and CompileCommand

Both flags enable tracing (and some control) of method compilation +
and require `-XX:+UnlockDiagnosticVMOptions`.

=== !

`java -XX:CompileCommand=help` will print all possible options. 

and `-XX:PrintAssemblyOptions=intel` will output assembly in Intel syntax, which I find easier to read, 

[%notitle]
=== disassembled null check folding

[source,nasm]
----
sub    $0x18,%rsp
mov    %rbp,0x10(%rsp)    ;*synchronization entry
                              ; - NullCheckFolding::nullCheckFolding@-1 (line 19)

add    $0x10,%rsp
pop    %rbp
test   %eax,0x16b74929(%rip)        # 0x00007f6cd3c86000
                                                ;   {poll_return}
retq
----

=== don't be a afraid of the dark

https://en.wikibooks.org/wiki/X86_Disassembly/Calling_Conventions[x86 Disassembly/Calling Conventions]

https://en.wikibooks.org/wiki/X86_Assembly/X86_Architecture#Addressing_modes[x86 Assembly/X86 Architecture/Addressing modes]

https://www.cs.princeton.edu/courses/archive/spr08/cos217/reading/ProgrammingGroundUp-1-0-lettersize.pdf[Programming from the Ground Up]

https://pacman128.github.io/static/pcasm-book.pdf[PC Assembly Language]


[role="highlight_title"]
== JIT is speculating

image::tumblr_nu2sk1SnbJ1snwccbo2_500.gif[background]

[%notitle]
=== JIT is speculating explained

JIT does not only compiles hot methods +
but also optimizes `hot paths`, +
so it speculates which part of your code is actually executed

(so, compilations don't dominate your application time)

=== uncommon traps

if JIT is speculating, what happens when it fails?

code takes branch that was not optimized?

type profile changes?

=== !

we fail into an uncommon trap

From HotSpot code (https://github.com/openjdk/jdk11/blob/37115c8ea4aff13a8148ee2b8832b20888a5d880/src/hotspot/share/runtime/deoptimization.hpp#L276[src/hotspot/share/runtime/deoptimization.hpp])

`Performs an uncommon trap for compiled code.
The top most compiler frame is converted into interpreter frames`

[%notitle]
=== uncommong trap - branch prediction

[source,java]
----
private static Object uncommonTrap(Object trap){
  if (trap != null) {
    System.out.println("I am being trapped!");
  }
  return null;
}

public static void main(String[] argv) {
  Object trap = null;
  for (int i = 0; i < 250; ++i) {
    for (int j = 0; j < CHUNK_SIZE; ++j) {
      trap = uncommonTrap(trap);
    }
    if (i == 200) {
      trap = new Object();
    }
  }
}
----

[%notitle]
=== type profile

or when type profile changes

[%notitle]
=== class hierarchy analisys

[source,java]
----
Calculator trap = new Sum(1, 1);
int result = 0;
for (int i = 0; i < 250; ++i) {
  for (int j = 0; j < CHUNK_SIZE; ++j) {
    result = trap.calculate();
  }
  if (i == 200) {
    System.out.println("I am being trapped!");
    trap = new Multiply(1, 1);
  }
}
----

== the loop

[ditaa]
----

+--------------+                       +-----------+
|  Interpreter |---------------------->|  Profile  |
+--------------+                       +-----------+
       ^                                     |
       |                                     |
       |                                     |
       |                                     |
       |                                     v
+--------------+                       +-----------+
|  Deoptimize  |<----------------------|  Compile  |
+--------------+                       +-----------+
----

=== deoptimization

* when speculation fails, catched by uncommon trap
* when CHA (class hierachy analisys) notices change in class hierarchy
* when method is no longer "hot", profile traces method frequency invocation

[role="highlight_title"]
== made not entrant

image::shall-not-pass.gif[background]

=== null check elimination

https://jpbempel.github.io/2013/09/03/null-check-elimination.html

=== constant folding and propagation

[%notitle]
=== constant folding and propagation example

[source,java]
----
public static long constantPropagation() {
    int x = 14;
    int y = 7 - x / 2;
    return y * (28 / x + 2);
}
----

=== pointer compare

[%notitle]
=== pointer example

[source,java]
----
public static int pointerCompare(Object obj) {
  Object anotherObj = new Object();
  if(obj == anotherObj){
    return 0;
  }
  return -1;
}
----

=== intrinsics

  an intrinsic function is a function available for use in a given
  programming language whose implementation is handled specially
  by the compiler

=== intrinsics vs native vs compiler

=== intrinsics in JVM

in a context of JVM it means somebody wrote specialized code which generates
assembler instructions, but they are not generated by C1 or C2, +
like vectorized (AVX2) operations, algorithms used for cryptography

[%notitle]
=== system arraycopy

[source,java]
----
private static long[] intrinsic(long[] arr){
    long[] destArr = new long[arr.length];
    System.arraycopy(arr, 0, destArr, 0, arr.length);
    return destArr;
}
----

[%notitle]
=== call stub

actually calls +
`stubGenerator_x86_64``_jlong_disjoint_arraycopy` +
stub, which is intrinsified code, generated by +
`generate_disjoint_long_oop_copy`

=== lock elission

[%notitle]
=== lock elission example

[source,java]
----
public static int lockEllision(int j) {
    Object lock = new Object();
    synchronized (lock) {
       j++;
      }
    return j;
}
----

[%notitle]
=== lock elission example

[source,java]
----
public Object lock = new Object();

public static int lockEllision(int j) {
    synchronized (lock) {
       j++;
      }
    return j;
}
----

== autovectorization

[source,java]
----
x1=y1+z1;
x2=y2+z2;
x3=y3+z3;
x4=y4+z4;
----

=== !

[source,java]
----
[y1,y2,y3,y4]+[z1,z2,z3,z4]
----

=== nothing new

SIMD (Single Instruction Multiple Data)

x86 SSE and AVX extensions +
add new instructions and wide registers

=== !

JVM has support for it for a long time +

but you have almost no control over it

=== intrinsics

`Arrays.fill()` +
`System.arrayCopy()`

these methods have their optimized stubs (not a JNI call)

=== C2 optimizations

JIT tries hard to recognize a patterns in you code and transform it using SIMD

hint: run below code with and without -XX:-UseSuperWord

[source,java]
----
float[] a = ...

for (int i = 0; i < a.length; i++) {
    a[i] = a[i] * a[i];
}
----

=== !

http://groups.csail.mit.edu/commit/papers/00/SLP-PLDI-2000.pdf[Exploiting Superword Level Parallelism with Multimedia InstructionSets] +
http://psy-lob-saw.blogspot.com/2015/04/on-arraysfill-intrinsics-superword-and.html[On Arrays.fill, Intrinsics, SuperWord and SIMD instructions] +
https://richardstartin.github.io/tags/vector[Richard Startin's Blog, Vectorisation]

=== !

[quote,Richard Starin,Vectorised Algorithms in Java]
Because AVX can reduce the processor frequency, it’s not always profitable to vectorise, so compilers employ cost models to decide when they should do so.
Such cost models require platform specific calibration, and sometimes C2 can get it wrong.

=== !

[quote,,JEP 414]
it seems that auto-vectorization of scalar code is not a reliable tactic for optimizing ad-hoc user-written loops unless the user pays unusually careful attention to unwritten contracts about exactly which loops a compiler is prepared to auto-vectorize. It is too easy to write a loop that fails to auto-vectorize, for a reason that no human reader can detect. Years of work on auto-vectorization, even in HotSpot, have left us with lots of optimization machinery that works only on special occasions. We want to enjoy the use of this machinery more often!

== throwing exceptions

[%notitle]
=== smoke and mirrors

*it's all smoke and mirrors*

[%notitle]
=== to understand JIT

if there is one thing you should take away from this chapter

there are people who understand JIT, really, and will make every effort
to make it produce code that is fast and CPU friendly

but sometimes they will fail to do so

[%notitle]
=== code read by humans

when your code is hard to read for humans, +
it will be even harder to read for compilers

=== small methods

small methods will be inline ealier, no need to wait for profiling data

`MaxTrivialSize` is 6 bytecode
`MaxInlineSize` is 35, compiler level 2 +

`private` and `final` are inlined

[%notitle]
=== too smart

and don't try be too smart, +
there can be only one smart guy in the room, +
and it is not you :)

focus on choosing the right data structures

=== warning

there is a limit on the size of compiled/native method +
and there is a limit on inline level

so choose your small and private methods wisely +
understand _hot path_ in your application code

and watch it with JITWatch

=== what is JITWatch

https://github.com/AdoptOpenJDK/jitwatch[jitwatch], +
is a log analyser and visualiser for HotSpot JIT compiler

=== more warnings

* deep call stacks are killers for inlining, I am looking at you Spring, JEE and RxJava too
* deep inheritance trees (aka `megamorphs`)
* unpredictable branches are bad for ya! "let's kill the if"
* using exceptions for control flow

== Q&A
