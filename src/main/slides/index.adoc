= JDK concurrency primitives
the hidden treasures
:idprefix:
:stem: asciimath
:backend: html
:source-highlighter: highlightjs
:revealjs_theme: night
:revealjs_history: true
:revealjs_controls: false
:imagesdir: images
:title-slide-background-image: pexels-photo-1252907.jpeg
:customcss: css/custom.css

== about me

* 0-9 sleep walker
* 9-17 performance engineer at http://neo4j.com[Neo4j]
* 17-19 founder of http://segfault.events[SegFault] and http://coredump.event[CoreDump]
* 19-21 working on new presentations
* 21-00 praising gods of bytecode at the Church of JVM

=== don't believe it

== agenda

* fast & furious introduction into JVM & threads
* what are concurrency primitives?

=== the treasure hunt

* locks
* queues
* barriers
* atomics
* executors

[role="highlight_title"]
=== show me the code

image::pexels-photo-169573.jpeg[background]

=== !

*I will, I promise*

== what's concurrency?

[quote,,]
    the ability of different parts or units of a program, algorithm,
    or problem to be executed out-of-order or in partial order,
    without affecting the final outcome

=== !

this doesn't say a word about threads

== !

[quote,,]
    this allows for parallel execution of the concurrent units.

== !

*Java's answer to concurrency*

threads?

right?

=== !

a simple model to implement +

[role="highlight_title"]
=== (joking)

image::https://media.giphy.com/media/4WkCTDMpgjsUU/giphy.gif[background]

=== (or almost impossible)

image::https://media.giphy.com/media/7q3By1tKdxjJ6/giphy.gif[background]

=== to get right

=== why?

=== !

threads are scheduled by OS, +
you don't have control over it

=== !

JVM tries to overcome it with +
https://wiki.openjdk.java.net/display/loom/Main[Project Loom] +
stay tuned +
there are working prototypes

=== !

you can communicate between threads +
only by shared memory +
you have to protect this memory +
so no two threads are modifying its content +
otherwise, you run into

[role="highlight_title"]
=== data race

obrazek

=== !

and when two threads can be scheduled +
at the same time on different CPU cores +
(and sockets) +
you add complexities of +
caches coherence and gory details of +
https://www.scss.tcd.ie/Jeremy.Jones/VivioJS/caches/MESIHelp.htm[MESI protocol], +
false sharing +
and all this weird stuff

=== !

all you have at your disposal are +
mutexes and conditional variables +

=== !

with these mechanisms, it is easy to get into +
deadlocks, +
livelocks, +
contention +
and starvation

=== synchronizations model quadrant

image::https://image.slidesharecdn.com/thinkingoutsidethesynchronisationquadrant-170627150206/95/thinking-outside-the-synchronisation-quadrant-11-638.jpg?cb=1498576348[]

=== !

image::https://image.slidesharecdn.com/fuscoconcurrency-151124181259-lva1-app6891/95/mario-fusco-comparing-different-concurrency-models-on-the-jvm-codemotion-milan-2015-39-638.jpg?cb=1448389354[alt]

[role="highlight_title"]
=== mutable shared state

image::https://media.giphy.com/media/8Fla28qk2RGlYa2nXr/giphy.gif[background]

=== !

is really the only part +
that's hard

=== !

but we cannot avoid it, +
because it is the only way +

=== !

image::https://media.giphy.com/media/kAC4na628Wljy/giphy.gif[background]

=== !

but you say actors, +
bullshit, +
you have mailbox +
(which at low level is a queue (a shared memory))

=== !

the only advice +
I can give you

stop thinking about threads, +
(if you don't have to)

=== !

because in a majority of cases +
threads +
are just wrong abstraction

== Java concurrency

`synchronized`

the basic Java model for concurrency +
is rather a simple mapping of +
POSIX threading,

=== !

we have threads, +
with priorities, +
which can be joinable or detached,

we have conditional variables

we have mutexes

threads can only communicate +
through shared variables

=== synchronized in depth

* fast path and slow path
* adaptive locking
* biased locking and contention
* object header and mutexes

=== fast path

is code generated by JIT compilers, +
so the whole thing happens in JVM, +
it uses spin-waiting with adaptive spinning +

this works great for non-contended scenarios +
with short critical sections

=== general advise

make critical sections as fast and short as possible

=== adaptive spinning

[quote,,]
  An optimization technique whereby a thread spins waiting for a change-of-state
  to occur (typically a flag that represents some event has occurred - such as
  the release of a lock) rather than just blocking until notified that the
  change has occurred. The "adaptive" part comes from the policy decisions
  that control how long the thread will spin until eventually deciding to block.

=== slow path

this is where JVM talks with OS +
through `pthreads`, +
which use `futex` syscall

=== biased locking

[source, java]
----
synchronized(this){ //monitorenter
  // do stuff
} // monitorexit;
----

what if this monitor will be aquired +
by the same thread, soon?

do we have to do `monitorexit`?

=== !

biased locking doesn't unlock, +
on `monitorexit` +
we say monitor is biased towards thread

=== !

but what happens when other thread wants to `monitoenter`? +
we revoke bias +
(which can be costly operation) +
(requires safepoint)

=== other tricks

=== lock coarsenning

[source,java]
----
synchronized(this){
  // some code
}
  // even more code
synchronized(this){
  // and more code
}
----

=== !

[source,java]
----
synchronized(this){
  // some code
  // even more code
  // and more code
}
----

=== lock ellision

[source,java]
----
Object lock = new Object();
synchronized(lock){
  // some code
  // even more code
  // and more code
}
----

=== escape analisys

`lock` is never shared with other threads +
(it doesn't escape) +
we can safely remove synchronized at runtime

=== people make mistakes

=== object header


[role="highlight_title"]
== concurrency primitives

image::blacksmith-tools-shop-rustic-162631.jpeg[background,size=cover]

== concurrency primitivites

* locks
* queues
* barries
* atomic variables
* executors

=== !

building blocks of concurrency since JDK 1.5, +
with many updates, fixes and additions +
in subsequent releases

=== !

this is my personal trauma, +
but I have a feeling, +
looking at many codebases +
that we stucked in +
`new Thread().start()` +
`Object.wait()` +
`Object.notify()`

== let's get back to basics

== object pool

image::pexels-photo-887821.jpeg[background]

=== requirements

* pool grows until it reaches maximum size
* it never blocks, even if doesn't have objects
* it never shrinks, unless we try to return object in invalid state
* it doesn't use locks (explicitly)


=== queues

Queue +
BlockingQueue +
Deque (double ended queue, used as FIFO and LIFO) +
TransferQueue

=== ordering guarantees

all concurrency primitives +
have precisely described ordering guarantees, +
using happens-before relation +
(http://research.microsoft.com/en-us/um/people/lamport/pubs/time-clocks.pdf[Time, Clocks and the Ordering of Events in a Distributed System])

=== !

[source, java]
----
var lock = new ReentrantLock();
x=0;
lock.lock();
y=x;
lock.unlock(); // this can't be reordered with lock()
x=1; // which means this can't be reordered too, transitive relation
----

=== READ THE DOCS

=== ArrayBlockingQueue

implementation is backed by an array, +
thus is _bounded queue_ +
and uses reentrant lock +
(we can make this queue fair) +
which serves as concurrency control,

=== !

it has blocking and non-blocking methods

`put()` to full queue or +
`take()` from empty queue blocks

`offer()` and `poll()` don't block +
unless you use timed versions

=== can we do it better?

== !

*non-blocking Lamport queue*


=== why volatile?

* all threads will see the same value (it doesn't necessarily mean you read from RAM every time, please read carefully https://shipilev.net/blog/2014/safe-public-construction/[Safe Publication and Safe Initialization in Java])
* access to volatile fields are synchronizing actions, can't be reordered with other synchronizing actions

=== !

* it gives you happens before ordering guarantees
* it has a side effect, access from the same cache line

=== things get complicated

* single producer - multiple consumers
* multiple producers - single consumer
* multiple producers - multiple consumers

=== !

if you want your eyes bleed, +
take a look at +
https://github.com/JCTools/JCTools[Java Concurrency Tools]

=== ConcurrentLinkedQueue

[quote,,Javadoc]
  An unbounded thread-safe queue based on linked nodes. This queue orders
  elements FIFO (first-in-first-out). The head of the queue is that element
  that has been on the queue the longest time. The tail of the queue is
  that element that has been on the queue the shortest time.
  New elements are inserted at the tail of the queue, and the queue retrieval
  operations obtain elements at the head of the queue.

=== !

[quote,,Javadoc]
   A ConcurrentLinkedQueue is an appropriate choice when many threads will share
   access to a common collection. Like most other concurrent collection
   implementations, this class does not permit the use of null elements.

=== !

[quote,,Javadoc]
  Memory consistency effects: As with other concurrent collections,
  actions in a thread prior to placing an object into a ConcurrentLinkedQueue
  happen-before actions subsequent to the access or removal of that element
  from the ConcurrentLinkedQueue in another thread.

=== SynchronousQueue

a really interesting beast which works simlary to CSP
(Communicating Sequential Processes) or Ada randezvous channels

each insertion operation must wait for remove operation by another thread

=== NOTE: queue identity

[quote,,Javadoc]
  Queue implementations generally do not define element-based versions of
  methods equals and hashCode but instead inherit the identity based
  versions from class Object, because element-based equality is not always
  well-defined for queues with the same elements but different
  ordering properties.

[role="highlight_title"]
== cache

image::pexels-photo-566862.jpeg[background]

=== requirements

* it has max size, but it is "soft" limit
* things get invalidated based on LRU algorithm

== executors

* single thread,
* fixed size,
* cached,
* scheduled,

=== task queues and policies

=== queues

you can use any implementation of BlockingQueue for task queue

=== !

* `ArrayBlockingQueue` for size limited task queues,
* `LinkedBlockingQueue` for unbounded task queues
* `SynchronousQueue` for direct tasks hands-off
* `PriorityBlockingQueue`, this is where you can manage priorities of tasks

=== policies

what happens when we are out of space in fixed size task queues?

it depends on `RejectedExecutionHandler` implementation,

=== !

* `AbortPolicy`
* `CallerRunsPolicy`
* `DiscardOldestPolicy`
* `DiscardPolicy`

=== !

[role="highlight_title"]
== actors

image::cary-grant-rosalind-russell-ralph-bellamy-actor-53370.jpeg[background]

=== requirements

* one actor behaviour executed at a time, thus actors are safe to mutate its state
* we are not going to implement other actors requirements

=== limitations

current scheduling doesn't differntiate +
between idle and busy actors, +
if actor doesn't has messages, +
shouldn't be scheduled

=== !

different queue implementations can let you modify actor behaviour,

* with PriorityQueue you can implement actor priorities
* with DelayQueue you can schedule actors with delays
* with SynchronousQueue you can implements blocking (don't do it)
* multiple producers - single consumer optimized queue (mailbox) implementation +
can make a whole lot of difference

=== !

understanding queue semantics is a key to success and world domination

== map/reduce

image::book-address-book-learning-learn-159751.jpeg[background]

=== requirements

I believe it doesn't need explanation

=== barries

aka synchronizers

=== CountDownLatch

`CountDownLatch` has a counter field, +
which we can decrement, +
it can then be used to block a calling thread +
until it’s been counted down to zero

=== !

It can be useful for parallel processing, +
we could instantiate the `CountDownLatch` +
with the same value for the counter +
as a number of threads

=== !

Then just call `countdown()` +
after each thread finishes, +
guaranteeing that a dependent thread +
calling `await()`will block +
until the worker threads are finished.

=== !

[source,java]
----
class Worker implements Runnable {

    private CountDownLatch countDownLatch;

    Worker(CountDownLatch countDownLatch) {
        this.countDownLatch = countDownLatch;
    }

    @Override
    public void run() {
        doSomeWork();
        countDownLatch.countDown();
    }
}
----

=== !

[source,java]
----
var executor = Executors.newCachedThreadPool();
var countDownLatch = new CountDownLatch(8);

for(int i=0;i<8;i++){
  executor.run(new Worker(countDownLatch));
}

countDownLatch.await();
----

=== CyclicBarrier

`CyclicBarriers` are used in programs +
in which we have a fixed number of threads +
that must wait for each other to reach +
a common point before continuing execution

=== !

The barrier is called cyclic because +
it can be re-used after +
the waiting threads are released.

=== !

[source,java]
----
class Mapper implements Runnable {

    private CyclicBarrier cyclicBarrier;

    Worker(CyclicBarrier cyclicBarrier) {
        this.cyclicBarrier = cyclicBarrier;
    }

    @Override
    public void run() {
        doSomeWork();
        cyclicBarrier.await();
    }
}
----

=== !

it can also have callback function, +
which is called when all threads, +
reach barrier

=== !

[source,java]
----
var workers = new ArrayList();
var barrier = new CyclicBarrier(8, new Reducer(workers));

workers.add(new Mapper(barrier));

// start all workers
----

=== !

* `CountDownLatch`: A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes.
* `CyclicBarrier`: A synchronization aid that allows a set of threads to all wait for each other to reach a common barrier point.

=== Phaser

The `Phaser` allows us to build logic +
in which threads need to wait on the barrier +
before going to the next step of execution.

=== !

We can coordinate multiple phases of execution, reusing a `Phaser` instance for
each program phase. Each phase can have a different number of threads waiting
for advancing to another phase.

=== !

To participate in the coordination, +
the thread needs to `register()` +
itself with the Phaser instance.


=== !

Thread signals that it arrived +
at the barrier by calling the +
`arriveAndAwaitAdvance()`, +
which is a blocking method.

=== !

When the number of arrived parties +
is equal to the number of registered parties, +
the execution of the program will continue, +
and the phase number will increase.

=== !

[source,java]
----
void runTasks(List<Runnable> tasks) {
   final Phaser phaser = new Phaser(1); // "1" to register self
   // create and start threads
   for (final Runnable task : tasks) {
     phaser.register();
     new Thread() {
       public void run() {
         phaser.arriveAndAwaitAdvance(); // await all creation
         task.run();
       }
     }.start();
   }

   // allow threads to start and deregister self
   phaser.arriveAndDeregister();
 }
----

=== !

One way to cause a set of threads +
to repeatedly perform actions +
for a given number of iterations +
is to override `onAdvance()` method.

=== !

[source,java]
----
final Phaser phaser = new Phaser() {
  protected boolean onAdvance(int phase, int registeredParties) {
    return phase >= iterations || registeredParties == 0;
  }
};
----

=== exchanger

is a like bidirectional `SynchronousQueue`, +
it has only one method +
(plus its timeout version) +
`V exchange(V)`,


=== !

[quote,,Javadoc]
  Each thread presents some object on entry to the exchange method, matches with a partner thread, and receives its partner's object on return

=== !

it has interesting property,

[quote,,Javadoc]
  For each pair of threads that successfully exchange objects via an Exchanger, actions prior to the exchange() in each thread happen-before those after a return from the corresponding exchange() in the other thread.

=== locks

Java locks are `synchronized` on steroids,

* they can be fair,
** fairness generally decreases throughput but reduces variability and avoids starvation






=== ReentrantLock

=== !

[source, java]
----
var lock = new ReentrantLock(true);
lock.lock();
// do something
lock.unlock();
----

=== !

* you can *try* to lock,

[source, java]
----
var lock = new ReentrantLock();
if(lock.tryLock()){
  try{
    // do something
  } finally{
    lock.unlock();
  }
} else{
  // do something else, or maybe retry
}
----

=== !

* locks are reentrant (mostly, read Javadocs), aka recursive mutexes which
allow the same thread to acquire multiple levels of ownership over the mutex object.

=== !

[source, java]
----
var lock = new ReentrantLock();
lock.lock();
lock.lock(); // this will work ;)
// do something
lock.unlock();
lock.unlock();

----

=== !

* more fine-grained control, plus all the stuff from `synchronized` like
conditional variables


=== ReentrantReadWriteLock

* `writeLock()`, returns exclusive lock
* `readLock()`, returns shared lock
* there can be only one thread which holds `writeLock()`, otherwise
everyone is welcomed to come in ;)

=== lock downgrading

because of this lock being reentrant we can do lock downgrading

=== !

[source,java]
----
var lock = new ReentrantReadWriteLock();
lock.writeLock().lock();
try{
  lock.readLock().lock();
  try{
    // read stuff, like linear scan for page
  } finally{
    lock.readLock().unlock();
  }
  // write stuff, like update page
} finally{
  lock.writeLock().unlock();
}
----


=== !

but you can't upgrade lock

=== starving threads

`ReentrantReadWriteLock` has some severe issues with starvation if not handled
properly (using fairness may help, but it may be an overhead and compromise throughput).
For example, a number of reads but very few writes can cause the writer thread
to fall into starvation (even with fairness enabled)

=== StampedLock

Gives you fine-grained control over mutual exclusions but is one of +
the hardest to master +
and performance increments +
are hard to predict, +
measure don't guess

=== !

`StampedLock` is made of a stamp and mode, +
where your lock acquisition method
returns a stamp, +
which is a long value used for unlocking within the final block.

=== !

If the stamp is ever zero, that means there's been a failure to acquire access. +
`StampedLock` is all about giving us a possibility to perform optimistic reads.


=== !

[source, java]
----
var lock = new StampedLock();
long stamp =  lock.writeLock();
try{
  // do stuff
}finally {
 lock.unlockWrite(stamp);
}
----

=== !

ok, but what about optimistic read?

[source,java]
----
var lock = new StampedLock();
var stamp = lock.tryOptimisticRead();
double currentX = x, currentY = y;
if (!lock.validate(stamp)) {
   stamp = lock.readLock();
   try {
     currentX = x;
     currentY = y;
   } finally {
      lock.unlockRead(stamp);
   }
}
----

=== !

[quote,,Javadocs]

  This mode can be thought of as an extremely weak version of a read-lock,
  that can be broken by a writer at any time. The use of optimistic mode for short read-only code segments often reduces contention and   improves throughput. However, its use is inherently fragile.

=== !

[quote,,Javadocs]
  Optimistic read sections
  should only read fields and hold them in local variables for later use after
  validation. Fields read while in optimistic mode may be wildly inconsistent,
  so usage applies only when you are familiar enough with data representations
  to check consistency and/or repeatedly invoke method validate().

=== !

[quote,,Javadocs]
  For example, such steps are typically required when first reading an
  object or array reference, and then accessing one of its fields,
  elements or methods.

=== summary

* no owning threads
* no reentrancy
* 3 modes of operation
** write
** read
** optimistic read

=== Deep dive

https://www.javaspecialists.eu/archive/Issue215.html[StampedLock Idioms]


=== are locks faster ?

it depends ;)

=== !

https://mechanical-sympathy.blogspot.com/2013/08/lock-based-vs-lock-free-concurrent.html[Lock-Based vs Lock-Free Concurrent Algorithms]
https://blog.takipi.com/java-8-stampedlocks-vs-readwritelocks-and-synchronized/[Java 8 StampedLocks vs. ReadWriteLocks and Synchronized]

== linked list

image::pexels-photo-147635.jpeg[background]

=== but first

Is this code thread safe?

[source, java]
----
i++;
----

=== short answer: NO

[source, nasm]
----
mov    0x2ee5(%rip),%eax # load from memory
add    $0x1,%eax # add value
mov    %eax,0x2edc(%rip) # write back
----

=== !

let's take a look at few counters examples

=== how does this work?

[source, nasm]
----
mov    $0x1,%eax
lock xadd %eax,0x2ede(%rip) # notice lock prefix
----

=== !

[quote,,]
  LOCK prefix feature guards a single instruction only and thus might
  hold other threads for the duration of that single instruction only.
  Since this is implemented by the CPU itself, it doesn’t require additional
  software efforts.

=== !

[quote,,]
  Therefore, the challenge of developing lock-free algorithms is not the removal
  of synchronization entirely, it boils down to reduce the critical section of
  the code to a single atomic operation which will be provided by the CPU itself.

=== LOCK prefix performance

https://spcl.inf.ethz.ch/Publications/.pdf/atomic-bench.pdf[Evaluating the Cost of Atomic Operations on Modern Architectures]

=== lock free programming

the short version

[source,java]
----
class Node<T>{
  T value;
  Node<T> next;
}

AtomicReference<Node> head = new AtomicReference<>(new Node());

void doLockFreeStyle(T value){
  Node<T> node = new Node<>(value);
  do{
    Node<T> current = head.get(); // grab current state
    node.next = current; // made calculations ;)
  } while(atomic.compareAndSet(current, node)); // try to commit, if fails retry
}
----

=== lock free programming idiom

this is a general pattern +
(be careful, this is pattern, doesn't fit everywhere)

=== !

* grab object state,
* make calculation (they need to be side effects free),
* try to commit by atomic operation on single memory address (value),
* if fails, repeat

=== atomic operations

* `compareAndSet()`
* `getAndSet()`
* `getAndIncrement()`

=== it is not all roses

"ABA problem"

=== !

[quote,,Wikipedia]
  ABA problem occurs during synchronization, when a location is read twice,
  has the same value for both reads, and "value is the same" is used to
  indicate "nothing has changed".

=== !
[quote,,Wikipedia]
  However, another thread can execute between
  the two reads and change the value, do other work, then change the value back,
  thus fooling the first thread into thinking "nothing has changed" even
  though the second thread did work that violates that assumption

=== workarounds

* tagged state reference: add extra "tag" or "stamp" bits to the quantity being considered
* intermediate nodes: a correct but expensive approach is to use intermediate nodes that are not data elements and thus assure invariants as elements are inserted and removed
* delayed reclamation: we have it for free in JVM, it is called GC, in generall it is easier to implement lock free stuff in runtimes with automatic memory management

=== tagged state reference

* AtomicMarkableReference
* AtomicStampedReference

=== !

Both implementations are based +
on additional object instance +
which holds both tag and reference,

=== WARNING

every atomic operation requires +
new instance of this "holder" +
(this is not like ultra GC friendly solution)

=== can we do it better?

yes, but not in JVM ;(

=== !

* double compare-and-swap (DCAS or CAS2), it uses two memory addresses (not necessarily contiguous), proven to be slow (Motorola 86k and PowerPC)

=== !

* double width compare-and-swap, https://www.felixcloutier.com/x86/CMPXCHG8B:CMPXCHG16B.html[CMPXCHG8B/CMPXCHG16B — Compare and Exchange Bytes], in managed runtimes with garbage collectors, almost impossible to implement +
(I believe, the more I think about it)

=== special cases

for really highly contended scenarios +
when we need a sum of numbers

* `LongAdder` and `DoubleAdder`
* `LongAccumulator`

=== !

Implementation keeps an array of counters that can grow on demand.

the more threads are calling `increment()`, +
the array will be longer +
each record in the array can be updated separately

=== !

On `sum()` we iterate over these arrays to calculate result.

== linked list

image::pexels-photo-147635.jpeg[background]


=== lock free structures

* Lamport queues,
* Harris linked list, https://timharris.uk/papers/2001-disc.pdf[A pragmatic implementation of non-blocking linked-lists, by Timothy L. Harris]
* lock free hash table, https://web.stanford.edu/class/ee380/Abstracts/070221_LockFreeHash.pdf[A Lock-Free Wait-Free Hash Table, by Cliff Click]

=== !

https://github.com/rigtorp/awesome-lockfree[Awesome Lock-Free]

I am sorry Martin and Nitsan, +
but if you really want +
to get into this stuff

=== !

watch what Herb's Sutter has to say, +
especially +
"Lock-Free Programming (or, Juggling Razor Blades)" +
and "atomic<> Weapons" <- this one is a killer it will melt your brain (second part)

=== A Pragmatic Implementation of Non-Blocking Linked-Lists

Proposed by Timothy L. Harris in 2001

=== !

[ditaa]
----
+----------+                                          +----------+
|  head    |                                          |  tail    |
+----------+      +----------+      +----------+      +----------+
|value|next|----->|value|next|----->|value|next|----->|value|next|
+----------+      +----------+      +----------+      +----------+
----

It has to special nodes (_sentinel_ nodes), +
called _head_ and _tail_, +
which are constant.

=== !

Insertion is straightforward: a new list cell is created

[ditaa]
----
+----------+      +----------+                           +----------+      +----------+
|value|next|----->|value|next|-------------------------->|value|next|----->|value|next|
+----------+      +----------+                           +----------+      +----------+
                                                              ^
                                                              |
                                        +----------+          |
                                        |value|next|----------+
                                        +----------+
----

=== !

and then introduced using single CAS operation on the next field of the proposed predecessor

[ditaa]
----
+----------+      +----------+                           +----------+      +----------+
|value|next|----->|value|next|                           |value|next|----->|value|next|
+----------+      +----------+                           +----------+      +----------+
                          ^                                    ^
                          |                                    |
                          |             +----------+           |
                          +-------------|value|next|-----------+
                                        +----------+
----

=== !

In this case the atomicity of the CAS ensures the the nodes either side of
the insertion have remained adjacent. This simple guarantee is insucient for
deletions within the list.

[pictures]

=== !

what can we do with delete operation?

[quote,,]
  delete ... then uses a two-stage process to perform the deletion. Firstly, the node is logically deleted
  by marking the reference contained in right node.next. Secondly, the node is physically deleted.

`AtomicMarkableReference` anyone?

=== !

https://www.cl.cam.ac.uk/research/srg/netos/papers/2001-caslists.pdf[A Pragmatic Implementation of Non-Blocking Linked-Lists] +
contains detailed description of this algorithm plus implementation if pseudocode.

=== another one?

=== !

http://people.csail.mit.edu/bushl2/rpi/portfolio/lockfree-grape/documents/lock-free-linked-lists.pdf[Lock-Free Linked Lists Using Compare-And-Swap]

proposes implementation based on auxiliary nodes

=== !

there are two types of nodes in list

[source, java]
----
interface Entry<T>{

}

class Aux<T> implements Entry<T>{
  Entry<T> next;
}

class Node<T> implements Entry<T>{
  T value;
  Aux<T> next;
}
----

=== !

there is special requirement that every `Node` +
has a predecessor and successor which are `Aux` nodes.

it is permitted that `Aux` node +
predecessor and successor is `Aux` +
(but we try to limit this
, this will degrade performance)
=== !

there is also a special structure used to traverse the list

[source,java]
----
class Cursor<T>{
  Entry<T> target;
  Entry<T> pre_aux;
  Node<T> pre_cell;

  boolean isValid(){
    return target==pre_aux;
  }
}
----

=== !

is cursors _is invalid_ it indicates that structure was concurrently modified

=== !

insertion requires adding both, +
cell and auxiliary nodes, +
and there is a CAS operation +
between auxiliary node and next cell

== what's next?
